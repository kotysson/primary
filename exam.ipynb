{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек и загрузка данных"
      ],
      "metadata": {
        "id": "5Y1SHnawu0e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "-2moqcC5vKSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
      ],
      "metadata": {
        "id": "CCRwrYb6vRxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f9ef37-1c87-47c0-b3e4-2d550553bc26"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 15:18:28--  https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94275 (92K) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]  92.07K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-12-25 15:18:28 (11.8 MB/s) - ‘shakespeare.txt’ saved [94275/94275]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "yOG5HhETvaVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "WEw4jS1QvZnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08883cd-9e62-45d5-935f-eef80f943bf9"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Экзаменационное задание"
      ],
      "metadata": {
        "id": "SZmaaWA9wCbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_file(file_path):\n",
        "  with open(file_path) as file:\n",
        "    text = file.read()\n",
        "    return text"
      ],
      "metadata": {
        "id": "Mz5pTfVFyPbA"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization function that gives us a list of not processed tokens\n",
        "def tokenize_text(text):\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "ABkVAtL_yGGD"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 1\n",
        "# text = 'From fairest creatures we desire increase'\n",
        "# a = tokenize_text(text)\n",
        "# a"
      ],
      "metadata": {
        "id": "RTitUthytzpa"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop_words with a list\n",
        "  # def remove_stopwords(tokens):\n",
        "  #   stops = set(stopwords.words('english'))\n",
        "  #   punctuation = ['.', ',', '!', '?', ':', '\\'s', '\\'', '\\(', '\\)', '\\(:', '\\:(']\n",
        "  #   filtered_tokens = []\n",
        "  #   for token in tokens:\n",
        "  #     if token not in stops and token not in punctuation:\n",
        "  #       filtered_tokens.append(token)\n",
        "  #   return filtered_tokens"
      ],
      "metadata": {
        "id": "mc1E1lQByLqU"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop_words with a regexp\n",
        "def remove_stopwords(tokens):\n",
        "  stops = set(stopwords.words('english'))\n",
        "  re_compile = re.compile('^[a-zA-Z]*\\Z')\n",
        "  filtered_tokens = []\n",
        "  for token in tokens:\n",
        "    if token not in stops and re_compile.search(token) is not None:\n",
        "      filtered_tokens.append(token)\n",
        "  return filtered_tokens"
      ],
      "metadata": {
        "id": "kLOo6hmBx5_-"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 2\n",
        "b = remove_stopwords(a)\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wswp_-IDvGTM",
        "outputId": "d5ace1bf-e704-421d-9e33-55fd9b2386f8"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fairest', 'creatures', 'desire', 'increase']"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency_analysis(file_path, n):\n",
        "    # open file\n",
        "    doc = open_file(file_path)\n",
        "    # tokenize text\n",
        "    tokens = tokenize_text(doc)\n",
        "    # remove stopwords\n",
        "    cleaned_tokens = remove_stopwords(tokens)\n",
        "    # calculate word frequencies\n",
        "    word_freq = Counter(cleaned_tokens)\n",
        "    most_freq = word_freq.most_common(n)\n",
        "    # print n word:frequencies pairs\n",
        "    print(f'Most common words in the text and their frequencies are as follows:')\n",
        "    for word, frequency in most_freq:\n",
        "      print(f'{word}: {frequency}')"
      ],
      "metadata": {
        "id": "2xEZ1YCazlJq"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'shakespeare.txt'\n",
        "word_frequency_analysis(file_path, 20)"
      ],
      "metadata": {
        "id": "BZQ_ThBCzjdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1048861d-8c02-4f27-dd35-09301b058a19"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words in the text and their frequencies are as follows:\n",
            "thy: 287\n",
            "thou: 234\n",
            "love: 188\n",
            "thee: 162\n",
            "doth: 88\n",
            "self: 78\n",
            "beauty: 70\n",
            "time: 69\n",
            "mine: 63\n",
            "shall: 59\n",
            "heart: 57\n",
            "sweet: 55\n",
            "eyes: 53\n",
            "art: 52\n",
            "yet: 51\n",
            "thine: 44\n",
            "fair: 43\n",
            "make: 43\n",
            "one: 43\n",
            "hath: 43\n"
          ]
        }
      ]
    }
  ]
}